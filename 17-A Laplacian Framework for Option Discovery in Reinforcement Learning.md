Proto-value functions (PVFs) implicitly define options.

Eigenpurposes are intrinsic reward functions that incentivize the agent to traverse the state space by following the principal directions of the learned representation. Each intrinsic reward function leads to a different eigenbehavior, which is the optimal policy for that reward function.

not all options capable of accelerating planning are useful for exploration. We show that options traditionally used in the literature to speed up planning hinder the agentsâ€™ performance if used for random exploration during learning.
