options—closed-loop policies for taking action over a period of time\
Human decision making routinely involves choice among temporally extended courses of action over a broad range of time scales.\
MDPs as they are conventionally conceived do not involve temporal abstraction or temporally extended action. They are based on a discrete time step: the unitary action taken at time t affects the state and reward at time t+1. There is no notion of a course of action persisting over a variable period of time. As a consequence, conventional MDP methods are unable to take advantage of the simplicities and efficiencies sometimes available at higher levels of temporal abstraction.\
the essence of analyzing temporally abstract actions in AI applications: goal directed behavior involves multiple overlapping scales at which decisions are made and modified\
Options consist of three components: a policy $\pi: S\times A \rightarrow [0,1]$, a termination condition $\beta: S^+ \rightarrow [0,1]$, and an initiation set $I \in S$. An option $I, \pi, \beta$ is available in state $s_t$ if and only if $s_t \in I$. If the option is taken, then actions are selected according to $\pi$ until the option terminates stochastically according to $\beta$.\
Sometimes it is useful for options to “timeout”, to terminate after some period of time has elapsed even if they have failed to reach any particular state. This is not possible with Markov options because their termination decisions are made solely on the basis of the current state, not on how long the option has been executing. To handle this and other cases of interest we allow semi-Markov options, in which policies and termination conditions may make their choices dependent on all prior events since the option was initiated. In general, an option is initiated at some time, say t , determines the actions selected for some number of steps, say k, and then terminates in $s_{t+k}$ . At each intermediate time $\tau$; $t\geq tau\less t+k$, the decisions of a Markov option may depend only on s, whereas the decisions of a semi-Markov option may depend on the entire preceding sequence, but not on events prior to $s_t$ . We call this sequence the history from $t$ to $\tau$ and denote it by $h_{t\tau}$.\
In semi-Markov options, the policy and termination condition are functions of possible histories
