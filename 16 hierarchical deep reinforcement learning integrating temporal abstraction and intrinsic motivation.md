Learning goal-directed behavior with sparse feedback from complex environments is a fundamental challenge for artificial intelligence. Learning in this setting requires the agent to represent knowledge at multiple levels of spatio-temporal abstractions and to explore the environment efficiently. Intrinsically motivated agents can explore new behavior for their own sake rather than to directly solve external goals.\
A top-level q-value function learns a policy over intrinsic goals, while a lower-level function learns a policy over atomic actions to satisfy the given goals. the top-level module learns a policy over options (subgoals) and the bottom-level module learns policies to accomplish the objective of each option.\
When the environment provides delayed rewards, we adopt a strategy to first learn ways to achieve intrinsically generated goals, and subsequently learn an optimal policy to chain them together. \
Sutton proposed the options framework, which involves abstractions over the space of actions. At each step, the agent chooses either a onestep “primitive” action or a “multi-step” action policy (option). we use entities or objects in the scene to parameterize goals in this environment. \
we built a custom pipeline to provide plausible object candidates. Note that the agent is still required to learn which of these candidates are worth pursuing as goals.
